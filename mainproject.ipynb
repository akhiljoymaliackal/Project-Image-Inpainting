{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mainproject.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akhiljoymaliackal/Project-Image-Inpainting/blob/master/mainproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "0p3Hi4-Qqztm",
        "colab_type": "code",
        "outputId": "95beb196-a004-4d28-f260-b3840131402a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python2.7/dist-packages (1.13.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.13.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (2.0.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.0.7)\n",
            "Requirement already satisfied: enum34>=1.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.1.6)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (3.7.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.0.9)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.13.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow) (0.33.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (0.7.1)\n",
            "Requirement already satisfied: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.0.post1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.11.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.16.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (0.7.1)\n",
            "Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow) (5.1.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.6.1->tensorflow) (40.9.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (0.15.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (3.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qrxNcWONry95",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wqHW0AKer6mr",
        "colab_type": "code",
        "outputId": "6163d7d3-4a39-4e46-e725-2a217193c933",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lt2WocxUv9bR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cH_FnyYqsG4K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_width=64\n",
        "input_height=64\n",
        "local_input_width=32\n",
        "local_input_height=32\n",
        "input_channel=3\n",
        "input_dim=100\n",
        "continue_training=False\n",
        "batch_size=64\n",
        "train_size=400\n",
        "Tc=100\n",
        "Td=1\n",
        "learning_rate=0.001\n",
        "momentum=0.5\n",
        "alpha=0.5\n",
        "margin=5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Pd-HEKPsO0f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkpoints_path='/content/gdrive/My Drive/Project/checkpoint'\n",
        "graph_path='/content/gdrive/My Drive/Project/graph'\n",
        "images_path='/content/gdrive/My Drive/Project/image'\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "juyligoDsXh2",
        "colab_type": "code",
        "outputId": "8e7aec63-90df-47a3-bfed-5d45d7cf054c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2746
        }
      },
      "cell_type": "code",
      "source": [
        "def batch_norm(input, name=\"batch_norm\"):\n",
        "\twith tf.variable_scope(name) as scope:\n",
        "\t\tinput = tf.identity(input)\n",
        "\t\tchannels = input.get_shape()[3]\n",
        "\n",
        "\t\toffset = tf.get_variable(\"offset\", [channels], dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n",
        "\t\tscale = tf.get_variable(\"scale\", [channels], dtype=tf.float32, initializer=tf.random_normal_initializer(1.0, 0.02))\n",
        "\n",
        "\t\tmean, variance = tf.nn.moments(input, axes=[0,1,2], keep_dims=False)\n",
        "\n",
        "\t\tnormalized_batch = tf.nn.batch_normalization(input, mean, variance, offset, scale, variance_epsilon=1e-5)\n",
        "\n",
        "\t\treturn normalized_batch\n",
        "\n",
        "def linear(input, output_size, name=\"linear\"):\n",
        "\tshape = input.get_shape().as_list()\n",
        "\n",
        "\twith tf.variable_scope(name) as scope:\n",
        "\t\tmatrix = tf.get_variable(\"W\", [shape[1], output_size], tf.float32, tf.random_normal_initializer(stddev=0.02))\n",
        "\t\tbias = tf.get_variable(\"bias\", [output_size], initializer=tf.constant_initializer(0.0))\n",
        "\n",
        "\t\treturn tf.matmul(input, matrix) + bias\n",
        "\n",
        "def conv2d(input, out_filter, padding, kernel=5, stride=2, name=\"conv2d\"):\n",
        "\tinput_shape = input.get_shape().as_list()\n",
        "\twith tf.variable_scope(name) as scope:\n",
        "\t\tw = tf.get_variable(\"w\", [kernel, kernel, input_shape[-1], out_filter], initializer=tf.random_normal_initializer(stddev=0.02))\n",
        "\t\tb = tf.get_variable(\"b\", [out_filter], initializer=tf.constant_initializer(0.0))\n",
        "\t\tconv = tf.nn.conv2d(input, w,strides=[1, stride, stride, 1],padding=padding)\n",
        "\n",
        "\t\tconv = tf.reshape(tf.nn.bias_add(conv, b), conv.get_shape())\n",
        "\n",
        "\t\treturn conv\n",
        "\n",
        "def deconv2d(input, out_shape, name=\"deconv2d\"):\n",
        "\tinput_shape = input.get_shape().as_list()\n",
        "\twith tf.variable_scope(name) as scope:\n",
        "\t\tw = tf.get_variable(\"w\", [4, 4, out_shape[-1], input_shape[-1]], initializer=tf.random_normal_initializer(stddev=0.02))\n",
        "\t\tb = tf.get_variable(\"b\", [out_shape[-1]], initializer=tf.constant_initializer(0.0))\n",
        "\t\tdeconv = tf.nn.conv2d_transpose(input, w,output_shape=out_shape,strides=[1, 2, 2, 1],padding=\"SAME\")\n",
        "\t\tdeconv = tf.reshape(tf.nn.bias_add(deconv, b), deconv.get_shape())\n",
        "\n",
        "\t\treturn deconv\n",
        "def dilate_conv2d(input, out_shape, rate, name=\"dilate_conv2d\"):\n",
        "\tinput_shape = input.get_shape().as_list()\n",
        "\twith tf.variable_scope(name) as scope:\n",
        "\t\tw = tf.get_variable(\"w\", [3, 3, input_shape[-1], out_shape[-1]])\n",
        "\t\tb = tf.get_variable(\"b\", [out_shape[-1]], initializer=tf.constant_initializer(0.0))\n",
        "\t\tdilate_conv = tf.nn.atrous_conv2d(input,w,rate=rate,padding=\"SAME\")\n",
        "\t\tdilate_conv = tf.reshape(tf.nn.bias_add(dilate_conv, b), dilate_conv.get_shape())\n",
        "\t\treturn dilate_conv\n",
        "\n",
        "        #return dilate_conv\n",
        "\n",
        "\n",
        "# In[28]:\n",
        "\n",
        "\n",
        "#ops.py\n",
        "def block_patch(input, margin=5):\n",
        "\tshape = input.get_shape().as_list()\n",
        "\n",
        "\t#create patch in random size\n",
        "\tpad_size = tf.random_uniform([2], minval=15, maxval=25, dtype=tf.int32)\n",
        "\tpatch = tf.zeros([pad_size[0], pad_size[1], shape[-1]], dtype=tf.float32)\n",
        "\n",
        "\th_ = tf.random_uniform([1], minval=margin, maxval=shape[0]-pad_size[0]-margin, dtype=tf.int32)[0]\n",
        "\tw_ = tf.random_uniform([1], minval=margin, maxval=shape[1]-pad_size[1]-margin, dtype=tf.int32)[0]\n",
        "\n",
        "\tpadding = [[h_, shape[0]-h_-pad_size[0]], [w_, shape[1]-w_-pad_size[1]], [0, 0]]\n",
        "\tpadded = tf.pad(patch, padding, \"CONSTANT\", constant_values=1)\n",
        "\n",
        "\tcoord = h_, w_\n",
        "\n",
        "\tres = tf.multiply(input, padded)\n",
        "\n",
        "\treturn res, padded, coord, pad_size\n",
        "\n",
        "def load_train_data():\n",
        "\tpaths = '/content/gdrive/My Drive/Project/dataset/*.jpg'\n",
        "\tdata_count = len(glob(paths))\n",
        "\n",
        "\tfilename_queue = tf.train.string_input_producer(tf.train.match_filenames_once(paths))\n",
        "\n",
        "\timage_reader = tf.WholeFileReader()\n",
        "\t_, image_file = image_reader.read(filename_queue)\n",
        "\timages = tf.image.decode_jpeg(image_file, channels=3)\n",
        "\n",
        "\timages = tf.image.resize_images(images ,[input_height, input_width])\n",
        "\timages = tf.image.convert_image_dtype(images, dtype=tf.float32) / 127.5 - 1\n",
        "\n",
        "\torig_images = images\n",
        "\timages, mask, coord, pad_size = block_patch(images, margin=margin)\n",
        "\tmask = tf.reshape(mask, [input_height, input_height, 3])\n",
        "\n",
        "\tmask = -(mask - 1)\n",
        "\timages += mask\n",
        "\n",
        "\torig_imgs, perturbed_imgs, mask, coord, pad_size = tf.train.shuffle_batch([orig_images, images, mask, coord, pad_size],batch_size=batch_size,capacity=batch_size*2,min_after_dequeue=batch_size)\n",
        "\treturn orig_imgs, perturbed_imgs, mask, coord, pad_size, data_count\n",
        "\n",
        "\n",
        "# In[29]:\n",
        "\n",
        "\n",
        "class network():\n",
        "    def __init__(self):\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.input_dim = input_dim \n",
        "\n",
        "        self.local_width, self.local_height = local_input_width, local_input_height\n",
        "\n",
        "        self.m = margin\n",
        "\n",
        "        self.alpha = alpha\n",
        "\n",
        "        #prepare training data\n",
        "        self.real_img, self.perturbed_img, self.mask, self.coord, self.pads, self.data_count = load_train_data()\n",
        "        # self.orig_img, self.test_img, self.test_mask, self.test_data_count = load_test_data(args)\n",
        "        \n",
        "        self.single_orig = tf.placeholder(tf.float32, (batch_size, input_height, input_width, 3))\n",
        "        self.single_test = tf.placeholder(tf.float32, (batch_size, input_height, input_width, 3))\n",
        "        self.single_mask = tf.placeholder(tf.float32, (batch_size, input_height, input_width, 3))\n",
        "\n",
        "        self.build_model()\n",
        "        self.build_loss()\n",
        "\n",
        "        #summary\n",
        "        self.recon_loss_sum = tf.summary.scalar(\"recon_loss\", self.recon_loss) \n",
        "        self.d_loss_sum = tf.summary.scalar(\"d_loss\", self.d_loss) \n",
        "        self.loss_all_sum = tf.summary.scalar(\"loss_all\", self.loss_all)\n",
        "        self.input_img_sum = tf.summary.image(\"input_img\", self.perturbed_img, max_outputs=5)\n",
        "        self.real_img_sum = tf.summary.image(\"real_img\", self.real_img, max_outputs=5)\n",
        "        \n",
        "        self.recon_img_sum = tf.summary.image(\"recon_img\", self.recon_img, max_outputs=5)\n",
        "        self.g_local_imgs_sum = tf.summary.image(\"g_local_imgs\", self.g_local_imgs, max_outputs=5)\n",
        "        self.r_local_imgs_sum = tf.summary.image(\"r_local_imgs\", self.r_local_imgs, max_outputs=5)\n",
        "\n",
        "    #structure of the model\n",
        "    def build_model(self):\n",
        "        def rand_crop(img, coord, pads):\n",
        "          cropped = tf.image.resize_images(tf.image.crop_to_bounding_box(img, coord[0]-self.m, coord[1]-self.m, pads[0]+self.m*2, pads[1]+self.m*2), (self.local_height, self.local_width))\n",
        "          return cropped\n",
        "\n",
        "        # uncomment to concatenate mask and masked input image\n",
        "        # self.perturbed_img = tf.concat([self.perturbed_img, self.mask], -1)\n",
        "\n",
        "        self.recon_img, self.g_nets = self.completion_net(self.perturbed_img, name=\"completion_net\")\n",
        "        self.recon_img = (1-self.mask)*self.real_img + self.mask*self.recon_img\n",
        "\n",
        "        self.test_res_imgs, _ = self.completion_net(self.single_test, name=\"completion_net\", reuse=True)\n",
        "        self.test_res_imgs = (1-self.single_mask)*self.single_orig + self.single_mask*self.test_res_imgs\n",
        "\n",
        "        self.r_local_imgs = []\n",
        "        self.g_local_imgs = [] \n",
        "        for idx in range(0,self.real_img.shape[0]):\n",
        "            r_cropped = rand_crop(self.real_img[idx], self.coord[idx], self.pads[idx])\n",
        "            g_cropped = rand_crop(self.recon_img[idx], self.coord[idx], self.pads[idx])\n",
        "            self.r_local_imgs.append(r_cropped)\n",
        "            self.g_local_imgs.append(g_cropped)\n",
        "\n",
        "\n",
        "        self.r_local_imgs = tf.convert_to_tensor(self.r_local_imgs)\n",
        "        self.g_local_imgs = tf.convert_to_tensor(self.g_local_imgs)\n",
        "        \n",
        "        #global discriminator setting\n",
        "        self.local_fake_d_logits, self.local_fake_d_net = self.local_discriminator(self.g_local_imgs, name=\"local_discriminator\")\n",
        "        self.local_real_d_logits, self.local_real_d_net = self.local_discriminator(self.r_local_imgs, name=\"local_discriminator\", reuse=True)\n",
        "\n",
        "        #local discriminator setting\n",
        "        self.global_fake_d_logits, self.global_fake_d_net = self.global_discriminator(self.recon_img, name=\"global_discriminator\")\n",
        "        self.global_real_d_logits, self.global_real_d_net = self.global_discriminator(self.real_img, name=\"global_discriminator\", reuse=True)\n",
        "\n",
        "        self.fake_d_logits = tf.concat([self.local_fake_d_logits, self.global_fake_d_logits], axis=1)\n",
        "        self.real_d_logits = tf.concat([self.local_real_d_logits, self.global_real_d_logits], axis=1)\n",
        "\n",
        "        self.fake_loss = linear(self.fake_d_logits, 1, \"fake_loss\")\n",
        "        self.real_loss = linear(self.real_d_logits, 1, \"real_loss\")\n",
        "\n",
        "        trainable_vars = tf.trainable_variables()\n",
        "        self.c_vars = []\n",
        "        self.d_vars = []\n",
        "        for var in trainable_vars:\n",
        "            if \"completion_net\" in var.name:\n",
        "                self.c_vars.append(var)\n",
        "            else:\n",
        "                self.d_vars.append(var)\n",
        "\n",
        "    #loss function\n",
        "    def build_loss(self):\n",
        "        def calc_loss(logits, label):\n",
        "            if label==1:\n",
        "                y = tf.ones_like(logits)\n",
        "            else:\n",
        "                y = tf.zeros_like(logits)\n",
        "            return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y))\n",
        "\n",
        "        self.fake_d_loss = calc_loss(self.fake_loss, 0)\n",
        "        self.real_d_loss = calc_loss(self.real_loss, 1)\n",
        "\n",
        "        #loss to train the discriminator\n",
        "        self.d_loss = self.alpha*(self.fake_d_loss + self.real_d_loss)\n",
        "\n",
        "        self.g_loss = calc_loss(self.fake_loss, 1)\n",
        "        \n",
        "        #mse loss in the paper\n",
        "        self.recon_loss = tf.reduce_mean(tf.nn.l2_loss(self.real_img - self.recon_img))\n",
        "        \n",
        "        self.loss_all = self.recon_loss + self.alpha*self.g_loss\n",
        "\n",
        "    # completion network \n",
        "    def completion_net(self, input, name=\"generator\", reuse=False):\n",
        "        input_shape = input.get_shape().as_list()\n",
        "        nets = []\n",
        "        with tf.variable_scope(name, reuse=reuse) as scope:\n",
        "            conv1 = conv2d(input, 64,kernel=5,stride=1,padding=\"SAME\",name=\"conv1\")\n",
        "            conv1 = batch_norm(conv1, name=\"conv_bn1\")\n",
        "            conv1 = tf.nn.relu(conv1)\n",
        "            \n",
        "            conv2 = conv2d(conv1, 128,kernel=3,stride=2,padding=\"SAME\",name=\"conv2\")\n",
        "            conv2 = batch_norm(conv2, name=\"conv_bn2\")\n",
        "            conv2 = tf.nn.relu(conv2)\n",
        "\n",
        "            conv3 = conv2d(conv2, 128,kernel=3,stride=1,padding=\"SAME\",name=\"conv3\")\n",
        "            conv3 = batch_norm(conv3, name=\"conv_bn3\")\n",
        "            conv3 = tf.nn.relu(conv3)\n",
        "\n",
        "            conv4 = conv2d(conv3, 256,kernel=3,stride=2,padding=\"SAME\",name=\"conv4\")\n",
        "            conv4 = batch_norm(conv4, name=\"conv_bn4\")\n",
        "            conv4 = tf.nn.relu(conv4)\n",
        "\n",
        "            conv5 = conv2d(conv4, 256,kernel=3,stride=1,padding=\"SAME\",name=\"conv5\")\n",
        "            conv5 = batch_norm(conv5, name=\"conv_bn5\")\n",
        "            conv5 = tf.nn.relu(conv5)\n",
        "\n",
        "            conv6 = conv2d(conv5, 256,kernel=3,stride=1,padding=\"SAME\",name=\"conv6\")\n",
        "            conv6 = batch_norm(conv5, name=\"conv_bn6\")\n",
        "            conv6 = tf.nn.relu(conv5)\n",
        "\n",
        "            #Dilated conv from here\n",
        "            dilate_conv1 = dilate_conv2d(conv6,[self.batch_size, conv6.get_shape()[1], conv6.get_shape()[2], 256],rate=2,name=\"dilate_conv1\")\n",
        "\n",
        "            dilate_conv2 = dilate_conv2d(dilate_conv1,[self.batch_size, dilate_conv1.get_shape()[1], dilate_conv1.get_shape()[2], 256],rate=4,name=\"dilate_conv2\")\n",
        "\n",
        "            dilate_conv3 = dilate_conv2d(dilate_conv2,[self.batch_size, dilate_conv2.get_shape()[1], dilate_conv2.get_shape()[2], 256],rate=8,name=\"dilate_conv3\")\n",
        "\n",
        "            dilate_conv4 = dilate_conv2d(dilate_conv3,[self.batch_size, dilate_conv3.get_shape()[1], dilate_conv3.get_shape()[2], 256],rate=16,name=\"dilate_conv4\")                                                                                              \n",
        "\n",
        "            #resize back\n",
        "            conv7 = conv2d(dilate_conv4, 256,kernel=3,stride=1,padding=\"SAME\",name=\"conv7\")\n",
        "            conv7 = batch_norm(conv7, name=\"conv_bn7\")\n",
        "            conv7 = tf.nn.relu(conv7)\n",
        "\n",
        "            conv8 = conv2d(conv7, 256,kernel=3,stride=1,padding=\"SAME\",name=\"conv8\")\n",
        "            conv8 = batch_norm(conv8, name=\"conv_bn8\")\n",
        "            conv8 = tf.nn.relu(conv8)\n",
        "\n",
        "            deconv1 = deconv2d(conv8, [self.batch_size, input_shape[1]/2, input_shape[2]/2, 128], name=\"deconv1\")\n",
        "            deconv1 = batch_norm(deconv1, name=\"deconv_bn1\")\n",
        "            deconv1 = tf.nn.relu(deconv1)\n",
        "\n",
        "            conv9 = conv2d(deconv1, 128,kernel=3,stride=1,padding=\"SAME\",name=\"conv9\")\n",
        "            conv9 = batch_norm(conv9, name=\"conv_bn9\")\n",
        "            conv9 = tf.nn.relu(conv9)\n",
        "\n",
        "            deconv2 = deconv2d(conv9, [self.batch_size, input_shape[1], input_shape[2], 64], name=\"deconv2\")\n",
        "            deconv2 = batch_norm(deconv2, name=\"deconv_bn2\")\n",
        "            deconv2 = tf.nn.relu(deconv2)\n",
        "\n",
        "            conv10 = conv2d(deconv2, 32,kernel=3,stride=1,padding=\"SAME\",name=\"conv10\")\n",
        "            conv10 = batch_norm(conv10, name=\"conv_bn10\")\n",
        "            conv10 = tf.nn.relu(conv10)\n",
        "\n",
        "            conv11 = conv2d(conv10, 3,kernel=3,stride=1,padding=\"SAME\",name=\"conv11\")\n",
        "            conv11 = batch_norm(conv11, name=\"conv_bn11\")\n",
        "            conv11 = tf.nn.tanh(conv11)\n",
        "\n",
        "            return conv11, nets\n",
        "\n",
        "    # D network from DCGAN\n",
        "    def local_discriminator(self, input, name=\"local_discriminator\", reuse=False):\n",
        "        nets = []\n",
        "        with tf.variable_scope(name, reuse=reuse) as scope:\n",
        "            conv1 = tf.contrib.layers.conv2d(input, 64, 5, 2,padding=\"VALID\",activation_fn=None,scope=\"conv1\")\n",
        "            conv1 = batch_norm(conv1, name=\"bn1\")\n",
        "            conv1 = tf.nn.relu(conv1)\n",
        "            nets.append(conv1)\n",
        "\n",
        "            conv2 = tf.contrib.layers.conv2d(conv1, 128, 5, 2,padding=\"VALID\",activation_fn=None,scope=\"conv2\")\n",
        "            conv2 = batch_norm(conv2, name=\"bn2\")\n",
        "            conv2 = tf.nn.relu(conv2)\n",
        "            nets.append(conv2)\n",
        "\n",
        "            conv3 = tf.contrib.layers.conv2d(conv2, 256, 5, 2,padding=\"VALID\",activation_fn=None,scope=\"conv3\")\n",
        "            conv3 = batch_norm(conv1, name=\"bn3\")\n",
        "            conv3 = tf.nn.relu(conv3)\n",
        "            nets.append(conv3)\n",
        "\n",
        "            conv4 = tf.contrib.layers.conv2d(conv3, 512, 5, 2,padding=\"VALID\",activation_fn=None,scope=\"conv4\")\n",
        "            conv4 = batch_norm(conv4, name=\"bn4\")                                                                                                                           \n",
        "            conv4 = tf.nn.relu(conv4)\n",
        "            nets.append(conv4)\n",
        "\n",
        "            conv5 = tf.contrib.layers.conv2d(conv4, 512, 5, 2,padding=\"VALID\",activation_fn=None,scope=\"conv5\")\n",
        "            conv5 = batch_norm(conv5, name=\"bn5\")                                                                                                                           \n",
        "            conv5 = tf.nn.relu(conv5)\n",
        "            nets.append(conv5)\n",
        "\n",
        "            flatten = tf.contrib.layers.flatten(conv5)\n",
        "\n",
        "            output = linear(flatten, 1024, name=\"linear\")\n",
        "\n",
        "            return output, nets\n",
        "\n",
        "\n",
        "\n",
        "    def global_discriminator(self, input, name=\"global_discriminator\", reuse=False):\n",
        "        nets = []\n",
        "        with tf.variable_scope(name, reuse=reuse) as scope:\n",
        "            conv1 = tf.contrib.layers.conv2d(input, 64, 5, 2,padding=\"VALID\",activation_fn=None,scope=\"conv1\")\n",
        "            conv1 = batch_norm(conv1, name=\"bn1\")\n",
        "            conv1 = tf.nn.relu(conv1)\n",
        "            nets.append(conv1)\n",
        "\n",
        "            conv2 = tf.contrib.layers.conv2d(conv1, 128, 5, 2,padding=\"VALID\",activation_fn=None,scope=\"conv2\")\n",
        "            conv2 = batch_norm(conv2, name=\"bn2\")\n",
        "            conv2 = tf.nn.relu(conv2)\n",
        "            nets.append(conv2)\n",
        "\n",
        "            conv3 = tf.contrib.layers.conv2d(conv2, 256, 5, 2,padding=\"VALID\",activation_fn=None,scope=\"conv3\")\n",
        "            conv3 = batch_norm(conv1, name=\"bn3\")\n",
        "            conv3 = tf.nn.relu(conv3)\n",
        "            nets.append(conv3)\n",
        "\n",
        "            conv4 = tf.contrib.layers.conv2d(conv3, 512, 5, 2,padding=\"VALID\",activation_fn=None,scope=\"conv4\")\n",
        "            conv4 = batch_norm(conv4, name=\"bn4\")                                                                                                                           \n",
        "            conv4 = tf.nn.relu(conv4)\n",
        "            nets.append(conv4)\n",
        "\n",
        "            conv5 = tf.contrib.layers.conv2d(conv4, 512, 5, 2,padding=\"VALID\",activation_fn=None,scope=\"conv5\")\n",
        "            conv5 = batch_norm(conv5, name=\"bn5\")                                                                                                                           \n",
        "            conv5 = tf.nn.relu(conv5)\n",
        "            nets.append(conv5)\n",
        "\n",
        "            conv6 = tf.contrib.layers.conv2d(conv5, 512, 5, 2,padding=\"VALID\",activation_fn=None,scope=\"conv6\")\n",
        "            conv6 = batch_norm(conv6, name=\"bn6\")                                                                                                                           \n",
        "            conv6 = tf.nn.relu(conv6)\n",
        "            nets.append(conv6)\n",
        "\n",
        "\n",
        "            flatten = tf.contrib.layers.flatten(conv6)\n",
        "\n",
        "            output = linear(flatten, 1024, name=\"linear\")\n",
        "\n",
        "\n",
        "            return output, nets\n",
        "\n",
        "\n",
        "# In[30]:\n",
        "\n",
        "\n",
        "#train\n",
        "def train(sess, model):\n",
        "    #Adam optimizers are used instead of AdaDelta\n",
        "    d_optimizer = tf.train.AdamOptimizer(learning_rate, beta1=momentum, name=\"AdamOptimizer_D\").minimize(model.d_loss, var_list=model.d_vars)\n",
        "    c_optimizer = tf.train.AdamOptimizer(learning_rate, beta1=momentum, name=\"AdamOptimizer_C\").minimize(model.recon_loss, var_list=model.c_vars)\n",
        "\n",
        "    global_optimizer = tf.train.AdamOptimizer(learning_rate, beta1=momentum, name=\"AdamOptimizer_C\").minimize(model.loss_all, var_list=model.c_vars)\n",
        "\n",
        "    epoch = 0\n",
        "    step = 0\n",
        "    global_step = 0\n",
        "\n",
        "    #saver\n",
        "    saver = tf.train.Saver()\n",
        "    if continue_training:\n",
        "        tf.local_variables_initializer().run()\n",
        "        last_ckpt = tf.train.latest_checkpoint(checkpoints_path)\n",
        "        saver.restore(sess, last_ckpt)\n",
        "        ckpt_name = str(last_ckpt)\n",
        "        print(\"Loaded model file from \" + ckpt_name)\n",
        "        epoch = int(ckpt_name.split('-')[-1])\n",
        "    else:\n",
        "        tf.global_variables_initializer().run()\n",
        "        tf.local_variables_initializer().run()\n",
        "\n",
        "\n",
        "\n",
        "    coord = tf.train.Coordinator()\n",
        "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
        "\n",
        "    #summary init\n",
        "    all_summary = tf.summary.merge([model.recon_loss_sum,\n",
        "                                    model.d_loss_sum,\n",
        "                                    model.loss_all_sum,\n",
        "                                    model.input_img_sum,\n",
        "                                    model.real_img_sum,\n",
        "                                    model.recon_img_sum,\n",
        "                                    model.g_local_imgs_sum,\n",
        "                                    model.r_local_imgs_sum])\n",
        "    writer = tf.summary.FileWriter(graph_path, sess.graph)\n",
        "\n",
        "    #training starts here\n",
        "\n",
        "    #first train completion network\n",
        "    while epoch <400:\n",
        "        print(\"hai\")\n",
        "        #Training Stage 1 (Completion Network)\n",
        "        if epoch < Tc:\n",
        "            summary, c_loss, _ = sess.run([all_summary, model.recon_loss, c_optimizer])\n",
        "            writer.add_summary(summary, global_step)\n",
        "            print(\"Epoch [%d] Step [%d] C Loss: [%.4f]\" % (epoch, step, c_loss))\n",
        "        elif epoch < Tc + Td:\n",
        "            #Training Stage 2 (Discriminator Network)\n",
        "            summary, d_loss, _ = sess.run([all_summary, model.d_loss, d_optimizer])\n",
        "            writer.add_summary(summary, global_step)\n",
        "            print (\"Epoch [%d] Step [%d] D Loss: [%.4f]\" % (epoch, step, d_loss))\n",
        "        else:\n",
        "            #Training Stage 3 (Completion Network)\n",
        "            summary, g_loss, _ = sess.run([all_summary, model.loss_all, global_optimizer])\n",
        "            writer.add_summary(summary, global_step)\n",
        "            print (\"Epoch [%d] Step [%d] C Loss: [%.4f]\" % (epoch, step, g_loss))\n",
        "\n",
        "\n",
        "        # Check Test image results every time epoch is finished\n",
        "        if step*batch_size >= model.data_count:\n",
        "            saver.save(sess, checkpoints_path + \"/model\", global_step=epoch)\n",
        "\n",
        "            #res_img = sess.run(model.test_res_imgs)\n",
        "\n",
        "            # save test img result\n",
        "            #img_tile(epoch, args, res_img)\n",
        "            step = 0\n",
        "            epoch += 1\n",
        "\n",
        "        step += 1\n",
        "        global_step += 1\n",
        "       \n",
        "      \n",
        "    \n",
        "     \n",
        "      \n",
        "\n",
        "\n",
        "    coord.request_stop()\n",
        "    coord.join(threads)\n",
        "    sess.close()\n",
        "    print(\"Done.\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    run_config = tf.ConfigProto()\n",
        "    run_config.gpu_options.allow_growth = True\n",
        "\n",
        "    #create graph, images, and checkpoints folder if they don't exist\n",
        "    if not os.path.exists(checkpoints_path):\n",
        "        os.makedirs(checkpoints_path)\n",
        "    if not os.path.exists(graph_path):\n",
        "        os.makedirs(graph_path)\n",
        "    if not os.path.exists(images_path):\n",
        "        os.makedirs(images_path)\n",
        "\n",
        "    with tf.Session(config=run_config) as sess:\n",
        "        model = network()\n",
        "\n",
        "        print('Start Training...')\n",
        "        train(sess, model)\n",
        "\n",
        "main()\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-6-a6c60fbd1cc2>:83: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:199: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:202: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From <ipython-input-6-a6c60fbd1cc2>:85: __init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.map(tf.read_file)`.\n",
            "WARNING:tensorflow:From <ipython-input-6-a6c60fbd1cc2>:99: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "Start Training...\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From <ipython-input-6-a6c60fbd1cc2>:391: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "hai\n",
            "Epoch [0] Step [0] C Loss: [27245.0938]\n",
            "hai\n",
            "Epoch [1] Step [1] C Loss: [20526.0332]\n",
            "hai\n",
            "Epoch [2] Step [1] C Loss: [11336.8848]\n",
            "hai\n",
            "Epoch [3] Step [1] C Loss: [16516.8574]\n",
            "hai\n",
            "Epoch [4] Step [1] C Loss: [12877.3770]\n",
            "hai\n",
            "Epoch [5] Step [1] C Loss: [12323.7744]\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "hai\n",
            "Epoch [6] Step [1] C Loss: [12860.6602]\n",
            "hai\n",
            "Epoch [7] Step [1] C Loss: [10301.5723]\n",
            "hai\n",
            "Epoch [8] Step [1] C Loss: [10693.1465]\n",
            "hai\n",
            "Epoch [9] Step [1] C Loss: [10367.0830]\n",
            "hai\n",
            "Epoch [10] Step [1] C Loss: [9876.2578]\n",
            "hai\n",
            "Epoch [11] Step [1] C Loss: [10862.4297]\n",
            "hai\n",
            "Epoch [12] Step [1] C Loss: [10967.2871]\n",
            "hai\n",
            "Epoch [13] Step [1] C Loss: [10125.6865]\n",
            "hai\n",
            "Epoch [14] Step [1] C Loss: [9387.1914]\n",
            "hai\n",
            "Epoch [15] Step [1] C Loss: [9121.4180]\n",
            "hai\n",
            "Epoch [16] Step [1] C Loss: [9148.8193]\n",
            "hai\n",
            "Epoch [17] Step [1] C Loss: [9543.0166]\n",
            "hai\n",
            "Epoch [18] Step [1] C Loss: [9794.3281]\n",
            "hai\n",
            "Epoch [19] Step [1] C Loss: [8777.6982]\n",
            "hai\n",
            "Epoch [20] Step [1] C Loss: [9086.0156]\n",
            "hai\n",
            "Epoch [21] Step [1] C Loss: [10057.3926]\n",
            "hai\n",
            "Epoch [22] Step [1] C Loss: [7494.6089]\n",
            "hai\n",
            "Epoch [23] Step [1] C Loss: [6860.4365]\n",
            "hai\n",
            "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Enqueue operation was cancelled\n",
            "\t [[{{node input_producer/input_producer_EnqueueMany}}]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-a6c60fbd1cc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-a6c60fbd1cc2>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Start Training...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-a6c60fbd1cc2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(sess, model)\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;31m#Training Stage 1 (Completion Network)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mTc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m             \u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecon_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_optimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch [%d] Step [%d] C Loss: [%.4f]\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "snOrQE--sdH5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0brBYm_Zs3uH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sho2hwuws_R1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UmKX434otECR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qO90fxqJsN7v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}